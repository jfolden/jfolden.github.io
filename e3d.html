<!DOCTYPE html>



 <html>

<head>
  
  <title>Brevin Tilmon</title>
  <link rel="stylesheet" type="text/css" href="style.css">
  <link rel="stylesheet" href="http://domain.tld/screen.css" type="text/css" media="Screen" />
  <link rel="stylesheet" href="http://domain.tld/mobile.css" type="text/css" media="handheld" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
  
  <link rel="shortcut icon" href=""/> 


  <style>
    #citation-text {
      font-family: "Courier New", Courier, monospace;
      background-color: #f8f8f8;
      padding: 1em;
      border: 1px solid #ccc;
      border-radius: 0px;
      overflow-x: auto; /* To handle long lines */
    }
  </style>
  
<style>
* {
  box-sizing: border-box;
}

/* Create two unequal columns that floats next to each other */
.column {
  float: left;
  padding-right: 15px;
  padding-bottom: 25px;
  height: 100%;
}

.columnR {
  float: right;
  padding-right: 0px;
  padding-bottom: 0px;
  height: 100%;
}

.left {
  width: 25%;
  padding-top: 4px;
}


.right {
  width: 75%;
}

.columnIm {
  float: left;
  padding-right: 2px;
  padding-bottom: 0px;
  height: 100%;
}

.columnImR {
  float: right;
  padding-right: 0px;
  padding-bottom: 0px;
  height: 100%;
}

.leftIm {
  width: 20%;
  padding-top: 4px;
  padding-right: 20px;
}

.rightIm {
  width: 80%;
  padding-right: 0px;
}

.leftInto {
  width: 50%;
  padding-top: 4px;
  padding-right: 20px;
}

.rightIntro {
  width: 50%;
  padding-top: 10px;
}

/* Clear floats after the columns */
.row:after {
  content: "";
  display: table;
  clear: both;
}

a{ 
  text-decoration: none; 
  color:#4682B4;
  font-weight: bold;
}

a.cvpra{
  text-decoration: none; 
  color:#4682B4;
  font-weight: normal;
}



.class1 {
  font-weight: bold;
}

.class2 {
  font-weight: normal;
}

a:hover{
  text-decoration: underline;
}

</style>

</head>

<body>
  <meta name="viewport" content="width=device-width, initial-scale=1">
<br><br>


<div class="containerCVPR">
  <!-- <div class="row">
    <div class="columnIm leftIntro">
      <h6 class="normal">Brevin Tilmon</h6> <br> 
    </div>
    <div class="columnR rightIntro" >


    </div>
  </div> -->

<h3 class="cvprTitle">Energy-Efficient Adaptive 3D Sensing</h3><br>
<h3 class="cvprConf">CVPR 2023</h3><br>
<h3 class="authors"><a class="cvpra" href="https://btilmon.github.io/">Brevin Tilmon</a><sup style="font-size:8.0pt">1</sup> &nbsp &nbsp <a class="cvpra" href="https://zhsun0357.github.io/">Zhanghao Sun</a><sup style="font-size:8.0pt">2</sup> &nbsp &nbsp <a  class="cvpra" href="https://focus.ece.ufl.edu/people/">Sanjeev Koppal</a><sup style="font-size:8.0pt">1</sup>  &nbsp &nbsp  <a  class="cvpra" href="https://yichengwu.github.io/">Yicheng Wu</a><sup style="font-size:8.0pt">3</sup><br>

    &nbsp &nbsp  <a  class="cvpra" href="https://sites.google.com/site/georgeevangelidis/">Georgios Evangelidis</a><sup style="font-size:8.0pt">3</sup> &nbsp &nbsp  <a  class="cvpra" href="https://www.linkedin.com/in/ramzi-zahreddine-42a09b87/">Ramzi Zahreddine</a><sup style="font-size:8.0pt">3</sup> &nbsp &nbsp  <a  class="cvpra" href="https://www.linkedin.com/in/krishnanguru/">Guru Krishnan</a><sup style="font-size:8.0pt">3</sup> &nbsp &nbsp  <a  class="cvpra" href="https://sizhuoma.netlify.app/">Sizhuo Ma</a><sup style="font-size:8.0pt">3</sup> &nbsp &nbsp  <a  class="cvpra" href="https://jianwang-cmu.github.io/">Jian Wang</a><sup style="font-size:8.0pt">3</sup>
</h3>
<h3 class="authors"><sup style="font-size:8.0pt">1</sup>University of Florida &nbsp &nbsp <sup style="font-size:8.0pt">2</sup>Stanford University &nbsp &nbsp <sup style="font-size:8.0pt">3</sup>Snap Inc. </h3>

<br>

<div class="image-container">
<a href="pubs/CVPR_2023_Energy_Efficient_Adaptive_3D_Sensing.pdf" target="_blank">
    <figure>
        <img src="pubs/e3dpdfTeaser.png" alt="Energy-Efficient Adaptive 3D Sensing" class="paper-image">
        <figcaption><a class="cvpra" href="https://btilmon.github.io/e3d.html">  Paper</a></figcaption>
    </figure>
</a>

<a href="pubs/CVPR_2023_Energy_Efficient_Adaptive_3D_Sensing_Supplementary.pdf" target="_blank">
  <figure>
      <img src="pubs/e3dsupppdfTeaser.png" alt="Energy-Efficient Adaptive 3D Sensing" class="paper-image">
      <figcaption><a class="cvpra" href="https://btilmon.github.io/e3d.html">  Supplementary</a></figcaption>
  </figure>
</a>

<a class="cvpra" href="https://github.com/btilmon/holoCu" target="_blank">
    <figure>
        <img src="img/holoCuTeaser2.png" alt="Energy-Efficient Adaptive 3D Sensing" class="paper-image">
        <figcaption><a class="cvpra" href="https://github.com/btilmon/holoCu">  Code</a></figcaption>
    </figure>
</a>


</div>
<br><br>

<video width="100%" controls>
  <source src="pubs/CVPR23_E3D_compressed.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<br><br><br>

<div style="text-align: center;">
  <img src="img/cvpr_demo.JPG" alt="Image description" width="25%" class="hw-image">
  <p>CVPR 2023 Real Time Demo</p>
</div>

<br> <br>
<h1 class="cvprSecTitle">Abstract</h1>
<h3 class="cvprSecBody">Active depth sensing achieves robust depth estimation but is usually limited by the sensing range. Naively increasing the optical power can improve sensing range but induces eye-safety concerns for many applications, including autonomous robots and augmented reality. In this paper, we propose an adaptive active depth sensor that jointly optimizes range, power consumption, and eye-safety. The main observation is that we need not project light patterns to the entire scene but only to small regions of interest where depth is necessary for the application and passive stereo depth estimation fails. We theoretically compare this adaptive sensing scheme with other sensing strategies, such as full-frame projection, line scanning, and point scanning. We show that, to achieve the same maximum sensing distance, the proposed method consumes the least power while having the shortest (best) eye-safety distance. We implement this adaptive sensing scheme with two hardware prototypes, one with a phase-only spatial light modulator (SLM) and the other with a micro-electro-mechanical (MEMS) mirror and diffractive optical elements (DOE). Experimental results validate the advantage of our method and demonstrate its capability of acquiring higher quality geometry adaptively. </h3>


<br> <br> <br>
<h1 class="cvprSecTitle">Method</h1>
<h3 class="cvprSecBody">We implement our adaptive 3D sensing framework in hardware as an active stereo system. In order to compare against existing 3D sensing techniques used in devices like Intel RealSense and Episcan3D, we emulate these devices on the spatial light modulator. See our code for our CUDA simulator of all 3D sensors. A spatial light modulator acts as a software-defined 3D sensor, enabling us to emulate the light redistribution properties of existing 3D sensors. We then adjust the camera exposure to finish the comparison. The image and video below demonstrates each emualted 3D sensor with the spatial light modulator. We scale the intensities for visualization. </h3>

<div style="text-align: center;">
  <img src="img/hardware.png" alt="Image description" width="100%" class="hw-image">
</div>

<br>
<br>

<div style="text-align: center;">
  <img src="img/sensors.png" alt="Image description" width="280" class="sensor-image">
</div>


<video width="100%" controls>
  <source src="img/e3dsensorteaserTitles.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>


<br> <br> 
<h1 class="cvprSecTitle">Results</h1>
<h3 class="cvprSecBody">Below we show comparisons of our adaptive sensor versus line scanning and full frame sensors. Notice the dots are much brighter for the adaptive sensor which results in better active stereo depth performance. We also show outdoor results under direct sunlight. Despite the very strong ambient light from the sun, we are able to adaptively reconstruct depth with our sensor. We can rely on passive stereo to reconstruct depth for the majority of the scene, and then fill in remaining missing depths in a small ROI with our adaptive sensor.</h3>

<div style="text-align: center;">
  <img src="img/resultsZoomCrop.png" class="hw-image" alt="Image description">
</div>

<br><br><br>

<div style="text-align: center;">
  <img src="img/outdoor.png" class="hw-image" alt="Image description">
</div>

<br>
<br><br>
<br>

<h1 class="cvprSecTitle">Real Time Demo</h1>
<h3 class="cvprSecBody">In this real time adaptive active stereo demo the textureless objects are detected  and then adaptively illuminated by the SLM. Notice the depth quality difference when the laser is off versus on. Please see our code for our CUDA-accelerated hologram generator. The demo runs self-contained on an embedded NVIDIA Jetson Nano with a 256 core GPU. Past a certain number of hologram points it is faster to just do Fourier Holography instead of our Fresnel Holography, but our Fresnel approach has significant speed ups for sparse holograms since the FFT runtime is constant for any number of dots and depends on the hologram size instead.</h3>

<video width="100%" controls>
  <source src="img/combined.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<br>
<br>
<h1 class="cvprSecTitle">Citation</h1>
<pre id="citation-text">
  @inproceedings{tilmon2023e3d,
    title     = {Energy-Efficient Adaptive 3D Sensing},
    author    = {Tilmon, Brevin and Sun, Zhanghao and Koppal, Sanjeev and Wu, Yicheng and Evangelidis, Georgios and Zahreddine, Ramzi and Krishnan, Guru and Ma, Sizhuo and Wang, Jian},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
  }
  </pre>
  

</div> 


</body>
</html>
